import sys
import boto3
import logging
import time
from pyspark import SparkContext
from pyspark.sql import Row
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import broadcast
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from datetime import datetime
#import for defining Schema
from pyspark.sql.types import *
from pyspark.sql.functions import *
######################GLUE import############################
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
##############################################################
# Declare variables
today_date=datetime.today().strftime("%Y%m%d")
today_timestamp=datetime.today().strftime("%Y%m%d%H%M")

s3_client = boto3.client('s3')
s3_resource = boto3.resource('s3')
sns_client = boto3.client('sns', region_name='us-east-1')
sts_client = boto3.client('sts')
sts_response = sts_client.get_caller_identity()
account_id = sts_response['Account']
TopicArn="arn:aws:sns:us-east-1:{}:Alert_Notification".format(account_id)

################################ Glue Parameters ############################

#getting Spark-parameters

args = getResolvedOptions(sys.argv, ['bucket_cleansed', 'bucket_enriched',
                                    'bucket_sftp', 'snowflake_flag',
                                    'JOB_NAME'])
bucket_cleansed = args['bucket_cleansed']
#aap-dl-customer-protected-cleansed-dev
bucket_enriched = args['bucket_enriched']
#aap-dl-customer-protected-cleansed-dev
bucket_sftp = args['bucket_sftp']
snowflake_flag = args['snowflake_flag']

sc = SparkContext()
glueContext = GlueContext(sc)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
spark = glueContext.spark_session 

#parameter declaration						   
prefix_in_1="ONEMAGNIFY/CDI_ENTITY_HIST_OPTINOUT"
prefix_in_2="EMAIL/OPT_CURRENT_STATUS"
prefix_in_3="BOUNCEEXCHANGE/BX_OPT_CURRENT"
output_path="BOUNCEEXCHANGE/BX_OPT_CURRENT"

####################################################################################
# spark specific settings
spark.conf.set("spark.sql.caseSensitive", "false")
spark.conf.set("spark.sql.sources.partitionColumnTypeInference.enabled", "true")
spark.conf.set("spark.sql.parquet.mergeSchema", "true")
spark.conf.set("spark.sql.hive.convertMetastoreParquet", "false")
spark.conf.set("spark.sql.parquet.cacheMetadata", "false")
spark.conf.set("spark.sql.files.maxPartitionBytes", 268435456)
spark.conf.set("spark.sql.parquet.mergeSchema", "true")
spark.conf.set("spark.files.maxPartitionBytes", 268435456)
spark.conf.set("spark.python.worker.reuse", "true")
spark.conf.set("spark.memory.offHeap.enabled", "true")
spark.conf.set("spark.dynamicAllocation.enabled", "true")
spark.conf.set("spark.sql.inMemorycolumnarStorage.compressed", "true")
spark.conf.set("spark.sql.inMemoryColumnarStorage.batchSize", 10000)

logger = logging.getLogger(__name__)
logging.getLogger().setLevel(logging.INFO)

environment = bucket_cleansed.split('-')[-1].upper()

now_ts = datetime.now()
curr_ts = now_ts.strftime("%Y-%m-%d %H:%M:%S")
TODAY_DATE = now_ts.strftime("%Y-%m-%d")
####################################################################################

# reading cdi_entity_history
file_src_1 = "s3://{}/{}".format(bucket_cleansed, prefix_in_1)
cdi_entity_hist_inc = spark.read.parquet(file_src_1)
cdi_entity_hist_inc.createOrReplaceTempView("cdi_entity_hist_inc")

cdi_entity_hist_inc_distinct= spark.sql("select distinct cstd_email from cdi_entity_hist_inc where source_code=34")
cdi_entity_hist_inc_distinct.createOrReplaceTempView("cdi_entity_hist_inc_distinct")
cdi_entity_hist_count=cdi_entity_hist_inc_distinct.count()
print("Source CDI History file count:{}".format(cdi_entity_hist_count))

# reading OPT_CURRENT_STATUS
file_src_2 = "s3://{}/{}".format(bucket_cleansed, prefix_in_2)
opt_current_status_df = spark.read.parquet(file_src_2)

opt_current_status_df.createOrReplaceTempView("opt_curr_status")
opt_current_count=opt_current_status_df.count()
print("Opt_current_status file count:{}".format(opt_current_count))


try:
    #joining CDI_ENTITY_HIST and OPT_CURRENT_STATUS based on opt_source_id
    join_df=spark.sql("select a.cstd_email,b.subscriber_type,b.subscription_id,b.opt_status,b.current_opt_date from \
                        cdi_entity_hist_inc_distinct a join opt_curr_status  b on trim(lower(a.cstd_email))=trim(lower(b.email_id))")
    
    join_df.createOrReplaceTempView("opt_incr_vw") 
    join_df_count=join_df.count()
    print("Records count after joining CDI History and OPT status file:{}".format(join_df_count))
    
    # reading  data set BX opt current status
    curr_bx_path = "s3://{}/{}".format(bucket_enriched, prefix_in_3)
    
    obj=s3_client.list_objects_v2(Bucket = bucket_enriched, Prefix = prefix_in_3)
    fileCount = obj['KeyCount']
    print("file Count in target:{}".format(fileCount))
    
    if fileCount > 1:
        try:
            curr_bx_opt_status = spark.read.option("mergeSchema", "true").parquet(curr_bx_path)
        except KeyError:
            print("***No data into target folder***")
            
        tmp_write_path = "s3://{}/TEMP/{}".format(bucket_enriched, prefix_in_3)
        curr_bx_opt_status.write.mode("overwrite").parquet(tmp_write_path)
        time.sleep(40)
        
        #reading dataframe
        curr_bx_opt_status_df = spark.read.parquet(tmp_write_path)
        curr_bx_opt_status.unpersist()
		
    else:
    	schema = StructType([StructField("cstd_email", StringType(), True), StructField("subscriber_type", StringType(), True),
                             StructField("subscription_id", StringType(), True),StructField("opt_status", IntegerType(), True),
                             StructField("current_opt_date", TimestampType(), True)])
    	curr_bx_opt_status_df = spark.createDataFrame(sc.emptyRDD(), schema)
    
    curr_bx_opt_status_df.createOrReplaceTempView("opt_curr_vw")
    curr_count=curr_bx_opt_status_df.count()
    print("Count of existing data in BX feed:{}".format(curr_count))
    
    #New Records
    df1=spark.sql("SELECT * FROM opt_incr_vw a WHERE NOT EXISTS (SELECT 1 FROM opt_curr_vw b WHERE a.cstd_email = b.cstd_email)")
    df1.createOrReplaceTempView("vw_new_ds")
    df1_count=df1.count()
    print("New reocrds count:{}".format(df1_count))
    
    #writeing incr data into temp location
    bx_opt_incr_path = "s3://{}/TEMP/{}".format(bucket_enriched, 'BX_OPT_INCR')
    df1.write.mode("overwrite").parquet(bx_opt_incr_path)
    
    #Modified records
    df2=spark.sql("select b.cstd_email,b.subscriber_type,b.subscription_id,b.opt_status,b.current_opt_date \
                   from opt_curr_vw a,opt_incr_vw b where a.cstd_email=b.cstd_email and a.opt_status!=b.opt_status")
    
    df2.createOrReplaceTempView("vw_changed_ds")
    df2_count=df2.count()
    print("Count of modified records:{}".format(df2_count))
    
    # appending modified data in temp location
    df2.write.mode("append").parquet(bx_opt_incr_path)
    
    #removing duplicate and union of letest reocrds and modified records
    df_out_load=spark.sql("select * from vw_new_ds union select * from vw_changed_ds union select * from opt_curr_vw")
    df_out_load.createOrReplaceTempView("vw_bx_out")
    
    df_out_load_fnl=spark.sql("select * from vw_bx_out where (cstd_email,current_opt_date) in (select cstd_email, max(current_opt_date) as current_opt_date from vw_bx_out group by cstd_email) order by cstd_email")
    df_out_load_fnl_count=df_out_load_fnl.count()
    print("Total final data count loaded in target for BX_OPT_CURRENT TABLE:{}".format(df_out_load_fnl_count))
    time.sleep(30)
    
    #Writing into parquet file into cleansed -BX Opt current data
    write_path_incr = "s3://{}/{}".format(bucket_enriched, output_path)
    print(write_path_incr)
    df_out_load_fnl.write.mode("overwrite").parquet(write_path_incr)
    print("BX_Opt_Current_Table updated with Incremental data Successfully")
    
    #Writing final df for mantaining History
    #write_path_hist = "s3://{}/{}".format(bucket_enriched, output_path+"_FEED_HISTORY")
    df_out_load_hist=df_out_load_fnl.withColumn("CREATE_DATE", lit(current_date()))
    #df_out_load_hist.write.mode("append").partitionBy("CREATE_DATE").parquet(write_path_hist)
    
    # Now extracting data for BX vendor from BX OPT CURRENT VIEW - sending only incremental data
    ven_exp_bx_feed=spark.read.parquet(bx_opt_incr_path)
    ven_exp_bx_feed=ven_exp_bx_feed.distinct()
    ven_cnt=ven_exp_bx_feed.count()
    print("Count of incremental (new & modified distinct records) for BX vendor:{}".format(ven_cnt))
    
    ven_exp_bx_feed.createOrReplaceTempView("vw_bx_out_fnl")
    ven_exp_bx_feed_to_cust=spark.sql("select cstd_email from vw_bx_out_fnl where opt_status=0")
    ven_exp_count=ven_exp_bx_feed_to_cust.count()
    print("final opted out email_id count send to vendor :{}".format(ven_exp_count))
    
    sftp_path="BounceExchange"
    enrith_path="BOUNCEEXCHANGE/VENDOR_EXPORT/BOUNCEEXCHANGE_EXPORT"
    vendor_folder_name=today_date
    vendor_file_name="BOUNCE_EXCHANGE_EXPORT_"+today_timestamp+".csv"
    
    #final_path=sftp_path+vendor_file_name
    write_path_feed="s3://{}/{}/{}".format(bucket_sftp, sftp_path,vendor_folder_name)
    
    ven_exp_bx_feed_to_cust.coalesce(1).write.mode("overwrite").csv(write_path_feed, header="true", sep="|")
    
    response = s3_client.list_objects_v2(Bucket = bucket_sftp, Prefix = sftp_path + "/" + vendor_folder_name + '/')['Contents']
    # print (response)
    obj_list =[]
    for x in response:
        obj_list.append(x.get('Key'))
    for obj in obj_list:
        if obj.split('/')[-1] != '_SUCCESS':
            dist_key= obj
    
    ######## Copy the part file to standard file name ########
    print("***loading export file into sftp path***")
    copy_source = {'Bucket': bucket_sftp, 'Key': dist_key}
    s3_client.copy_object(CopySource=copy_source, Bucket=bucket_sftp, Key=sftp_path + "/" + vendor_folder_name + "/" + vendor_file_name)
    
    print("***loading export file into enrithed path***")
    s3_client.copy_object(CopySource=copy_source, Bucket=bucket_enriched, Key=enrith_path + "/" + vendor_folder_name + "/" + vendor_file_name)
    
    ######### Delete the temporary files anf folders ##########
    for obj in obj_list:
        # print (obj)
        s3_client.delete_object(Bucket=bucket_sftp, Key=obj)
    
    print("Bounce Exchange vendor feed load is loaded Successfully")
    
    ###### Adding snowflake part ######
    
    if snowflake_flag == 'Y':
        env_var=bucket_cleansed.split('-')[-1]
        SF_DATABASE = "AAP_"+env_var.upper()+"_DB1"
        bucket_snow_flake = "aap-snowflake-"+env_var
        bucket_de_artifacts="aap-de-artifacts-"+env_var
        SNOWFLAKE_STAGE = "@S3_AAP_"+env_var.upper()+"_STAGE1"
        SNOWFLAKE_ROLE = "AAP_"+env_var.upper()+"_DB1_ROLE"
        ARTIFACT_BUCKET = bucket_de_artifacts
        SNOWFLAKE_SCHEMA = "PUBLIC"
        SNOWFLAKE_ACCOUNT = "aap.us-east-1"
        SF_WAREHOUSE = "LOAD_WH"
        AWS_REGION = "us-east-1"
        SQL_FILE ="VENDOR_EXPORT/BX_VENDOR_FEED.sql"
        
        #Writing into parquet file in snowflake_path
        prefix_snowflake=output_path
        snowflake_path="s3://{}/{}".format(bucket_snow_flake, prefix_snowflake)
        df_out_load_hist.write.mode("overwrite").partitionBy("CREATE_DATE").parquet(snowflake_path)
        
        #loading export data into snowflake
        prefix_snowflake_export="VENDOR_EXPORT_BOUNCE_EXCHANGE"
        snowflake_path_export="s3://{}/{}".format(bucket_snow_flake, prefix_snowflake_export)
        ven_exp_bx_feed_to_cust=ven_exp_bx_feed_to_cust.withColumn("CREATE_DATE", lit(current_date()))
        ven_exp_bx_feed_to_cust.write.mode("overwrite").partitionBy("CREATE_DATE").parquet(snowflake_path_export)
        
        df_out_load_fnl.unpersist()
        print("***Data loaded in snowflake bucket***")
        
        batch_client = boto3.client('batch', region_name='us-east-1')
        response = batch_client.submit_job(
        jobName='BX_VENDOR_FEED_JB',
        jobQueue='datawarehouse-pipeline-queue',
        jobDefinition='datawarehouse-pipeline',
        parameters={
              'database': SF_DATABASE,
              'wareHouse': SF_WAREHOUSE,
              'sqlFile': SQL_FILE,
              'region': AWS_REGION,
              'artifactBucket': ARTIFACT_BUCKET,
              'snowflakeStage': SNOWFLAKE_STAGE,
              'snowflakeAccount': SNOWFLAKE_ACCOUNT,
              'snowflakeRole': SNOWFLAKE_ROLE,
              'snowflakeSchema': SNOWFLAKE_SCHEMA
            }
        )
        print("***Data loaded in snowflak database***")
        
except Exception as e:
    print ('Error on line {}'.format(sys.exc_info()[-1].tb_lineno))
    print("Glue script failed with error : " , e)
    sns_client.publish(Subject="Environment: {}; AWS Glue Job Failed for {}".format(environment,prefix_snowflake), Message="The DE_{} job has failed. Error on line {}. Error Message:{}".format(prefix_snowflake.split('/')[-1], sys.exc_info()[-1].tb_lineno, e), TopicArn=TopicArn)
    raise(e)
